{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN2ptvsHpj4C"
      },
      "source": [
        "# Нейронні мережі\n",
        "\n",
        "## Опис\n",
        "\n",
        "В цій лабораторній ви будете працювати з набором даних [California Housing](https://www.kaggle.com/datasets/camnugent/california-housing-prices?resource=download), що використовується [Google](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) і OpenAI.\n",
        "\n",
        "## Підготовка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUgGjWkIpj4J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install pandas numpy seaborn matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Завантаження даних\n",
        "\n",
        "Завантажемо окремі файли `.csv` і створемо наступні два `DataFrame`:\n",
        "* `train_df`, який містить навчальний набір\n",
        "* `test_df`, який містить тестовий набір"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index)) # перемішуємо приклади\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ehCgIRjTxy"
      },
      "source": [
        "## Попередня обробка даних\n",
        "Перед моделюванням ми створимо шари попередньої обробки даних, використавши три ознаки:\n",
        "\n",
        "* широта (`latitude`) X довгота (`longitude`) (комбінована ознака)\n",
        "* середній дохід (`median_income`)\n",
        "* населення (`population`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EkNAQhnjSu-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Normalization, Discretization, HashedCrossing, Concatenate\n",
        "\n",
        "inputs = {\n",
        "    'latitude': Input(shape=(1,), dtype=tf.float32, name='latitude'),\n",
        "    'longitude': Input(shape=(1,), dtype=tf.float32, name='longitude'),\n",
        "    'median_income': Input(shape=(1,), dtype=tf.float32, name='median_income'),\n",
        "    'population': Input(shape=(1,), dtype=tf.float32, name='population')\n",
        "}\n",
        "\n",
        "# Створимо шар Normalization, щоб нормалізувати дані median_income\n",
        "median_income = Normalization(name='normalization_median_income', axis=None)\n",
        "median_income.adapt(train_df['median_income'])\n",
        "median_income = median_income(inputs.get('median_income'))\n",
        "\n",
        "# Створимо шар Normalization, щоб нормалізувати дані population.\n",
        "population = Normalization(name='normalization_population', axis=None)\n",
        "population.adapt(train_df['population'])\n",
        "population = population(inputs.get('population'))\n",
        "\n",
        "# Створимо список чисел, що представляють межі сегментів для широти.\n",
        "# Оскільки ми використовуємо шар нормалізації, значення широти та довготи будуть приблизно в діапазоні від -3 до 3 (що відповідає Z-score).\n",
        "# Ми створимо 20 сегментів, для чого необхідно мати 21 межу (отже, 20+1).\n",
        "latitude_boundaries = np.linspace(-3, 3, 20+1)\n",
        "\n",
        "# Створимо шар Normalization, щоб нормалізувати дані latitude\n",
        "latitude = Normalization(name='normalization_latitude', axis=None)\n",
        "latitude.adapt(train_df['latitude'])\n",
        "latitude = latitude(inputs.get('latitude'))\n",
        "\n",
        "# Створимо рівень Discretization, щоб розділити дані latitude на сегменти.\n",
        "latitude = Discretization(bin_boundaries=latitude_boundaries, name='discretization_latitude')(latitude)\n",
        "\n",
        "# Створимо список чисел, що представляють межі сегментів для довготи.\n",
        "longitude_boundaries = np.linspace(-3, 3, 20+1)\n",
        "\n",
        "# Створимо шар Normalization, щоб нормалізувати дані longitude\n",
        "longitude = Normalization(name='normalization_longitude', axis=None)\n",
        "longitude.adapt(train_df['longitude'])\n",
        "longitude = longitude(inputs.get('longitude'))\n",
        "\n",
        "# Створимо рівень Discretization, щоб розділити дані longitude на сегменти.\n",
        "longitude = Discretization(bin_boundaries=longitude_boundaries, name='discretization_longitude')(longitude)\n",
        "\n",
        "# Об’єднаємо ознаки широти та довготи в один one-hot вектор.\n",
        "# num_bins можна регулювати: вищі значення покращують точність, менші значення покращують продуктивність\n",
        "feature_cross = HashedCrossing(\n",
        "    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n",
        "    output_mode='one_hot',\n",
        "    name='cross_latitude_longitude')([latitude, longitude])\n",
        "\n",
        "# Об’єднаємо наші вхідні дані в один шар.\n",
        "preprocessing_layers = Concatenate()([feature_cross, median_income, population])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Модель лінійної регресії як базис\n",
        "\n",
        "Перш ніж створювати нейронну мережу, знайдемо базову помилку, створивши просту модель лінійної регресії, яка буде використовувати створені нами шари попередньої обробки.\n",
        "\n",
        "> Примітка: створимо її викорситавши нейронну мережу з 1 нейроном"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF0BFRXTOeR3"
      },
      "outputs": [],
      "source": [
        "def plot_the_loss_curve(epochs, mse_training, mse_validation):\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, mse_training, label=\"Training Loss\")\n",
        "  plt.plot(epochs, mse_validation, label=\"Validation Loss\")\n",
        "\n",
        "  merged_mse_lists = mse_training.tolist() + mse_validation\n",
        "  highest_loss = max(merged_mse_lists)\n",
        "  lowest_loss = min(merged_mse_lists)\n",
        "  top_of_y_axis = highest_loss * 1.03\n",
        "  bottom_of_y_axis = lowest_loss * 0.97\n",
        "\n",
        "  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6kEeb_spj4P"
      },
      "outputs": [],
      "source": [
        "# Створимо шари Normalization для нормалізації даних median_house_value.\n",
        "# Оскільки median_house_value є нашою міткою (тобто цільовим значенням, яке ми прогнозуємо), ці шари не будуть додані до нашої моделі.\n",
        "train_median_house_value_normalized = Normalization(axis=None)\n",
        "train_median_house_value_normalized.adapt(\n",
        "    np.array(train_df['median_house_value']))\n",
        "\n",
        "test_median_house_value_normalized = Normalization(axis=None)\n",
        "test_median_house_value_normalized.adapt(\n",
        "    np.array(test_df['median_house_value']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoGBGyJYpj4Q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "\n",
        "# Наступні змінні є гіперпараметрами.\n",
        "learning_rate = 0.01\n",
        "epochs = 15\n",
        "batch_size = 1000\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Розділимо оригінальний навчальний набір на скорочений навчальний набір і набір перевірки.\n",
        "validation_split = 0.2\n",
        "\n",
        "# Створимо Dense вихідний шар з 1 нейроном\n",
        "dense_output = Dense(units=1, name='dense_output')(preprocessing_layers)\n",
        "\n",
        "# Визначимо словник, який ми передамо до конструктора моделі.\n",
        "outputs = {\n",
        "    'dense_output': dense_output\n",
        "}\n",
        "\n",
        "# Створимо і скомпілюємо просту модель лінійної регресії\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Обʼєднаємо шари в модель, яку зможе виконати TensorFlow.\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mean_squared_error\", metrics=[MeanSquaredError()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgGfQguIpj4Q"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "# Навчимо модель на нормалізованому навчальному наборі.\n",
        "# Розділимо набір даних на ознаки та мітку\n",
        "features = {name:np.array(value) for name, value in train_df.items()}\n",
        "label = train_median_house_value_normalized(np.array(features.pop(label_name)))\n",
        "\n",
        "history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                    epochs=epochs, shuffle=True, validation_split=validation_split)\n",
        "\n",
        "# Візьмемо дані, які будуть корисні для побудови кривої втрат.\n",
        "epochs = history.epoch\n",
        "hist = pd.DataFrame(history.history)\n",
        "mse = hist[\"mean_squared_error\"]\n",
        "\n",
        "plot_the_loss_curve(epochs, mse, hist[\"val_mean_squared_error\"])\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
        "test_label = test_median_house_value_normalized(test_features.pop(label_name))\n",
        "print(\"\\n Оцінінемо модель лінійної регресії на тестовому наборі даних:\")\n",
        "model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Задача 1: Побудова багатошарової нейронної мережі\n",
        "\n",
        "Побудуйте багатошарову нейронну мережу, що складається з 2-х схованих Dense шарів і одного вихідного Dense шару.\n",
        "\n",
        "* 1-й схований шар повинен мати 20 нейронів, активаційну функцію `relu`, імʼя `hidden_dense_layer_1` та в якості вхідного шару використовувати наш `preprocessing_layers`.\n",
        "* 2-й схований шар повинен мати 12 нейронів, активаційну функцію `relu`, імʼя `hidden_dense_layer_2` та в якості вхідного шару використовувати 1-й схований шар.\n",
        "* Вихідний шар повинен мати таку ж конфігурацію як в попередній моделі регресії та в якості вхідного шару використовувати 2-й схований шар."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "nj3v5EKQFY8s"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "\n",
        "# Наступні змінні є гіперпараметрами.\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "batch_size = 1000\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Розділимо оригінальний навчальний набір на скорочений навчальний набір і набір перевірки.\n",
        "validation_split = 0.2\n",
        "\n",
        "hidden_dense_layer_1 = Dense(units=20, activation='relu', name='hidden_dense_layer_1')(preprocessing_layers)\n",
        "hidden_dense_layer_2 = Dense(units=12, activation='relu', name='hidden_dense_layer_2')(hidden_dense_layer_1)  #dense_output\n",
        "dense_output = tf.keras.layers.Dense(units=1, name='dense_output')(hidden_dense_layer_2)\n",
        "\n",
        "# Визначимо словник, який ми передамо до конструктора моделі.\n",
        "outputs = {\n",
        "    'dense_output': dense_output\n",
        "}\n",
        "\n",
        "# Створимо і скомпілюємо просту модель лінійної регресії\n",
        "multi_layer_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Обʼєднаємо шари в модель, яку зможе виконати TensorFlow.\n",
        "multi_layer_model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mean_squared_error\", metrics=[MeanSquaredError()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRg3JG4ypj4S"
      },
      "outputs": [],
      "source": [
        "\n",
        "multi_layer_history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                                epochs=epochs, shuffle=True, validation_split=validation_split)\n",
        "\n",
        "# Візьмемо дані, які будуть корисні для побудови кривої втрат.\n",
        "multi_layer_epochs = multi_layer_history.epoch\n",
        "multi_layer_hist = pd.DataFrame(multi_layer_history.history)\n",
        "multi_layer_mse = multi_layer_hist[\"mean_squared_error\"]\n",
        "\n",
        "plot_the_loss_curve(multi_layer_epochs, multi_layer_mse, multi_layer_hist[\"val_mean_squared_error\"])\n",
        "\n",
        "print(\"\\n Оцінінемо багатошарову модель на тестовому наборі даних:\")\n",
        "multi_layer_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWbP12eRpj4S"
      },
      "source": [
        "Є дуже велика ймовірність, що складна модель перетренерується на тренувальному наборі і буде мати велику помилку на перевірочному. Тому..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5IKmk7D49_n"
      },
      "source": [
        "## Задача 2: Оптимізація нейронної мережі\n",
        "Поекспериментуйте з кількістю шарів нейронної мережі та кількістю вузлів у кожному шарі. В ідеалі необхідно досягти обидві наступні цілі:\n",
        "\n",
        "* Знизити втрати на тестовому наборі.\n",
        "* Мінімізувати загальну кількість вузлів у нейронній мережі.\n",
        "\n",
        "Загалом, ці дві цілі можуть суперечити одна одній і треба знаходити компроміс, однак в нашому випадку якщо було перетренування мережі то все повинно бути добре.\n",
        "\n",
        "Варіантів може бути багато. Але є наступні думки:\n",
        "* Три шари можливо забагато і модель перетренеровується, що не дає досягти оптимального результату. Можливо два шари буде достатньо для компромісу.\n",
        "* Використовуючи два шари можна поексперементувати з кількістю нейронів, наприклад 10 в першому шарі і 6 в другому, або 6 в першому і 4 в другому. Але можна і навпаки збільшити кількість нейронів.\n",
        "\n",
        "> Примітка: Загальний код можна скопіпастити з попередніхприкладів/завдань"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyeSeMFepj4T"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "\n",
        "\n",
        "\n",
        "      #10 в першому шарі і 6 в другому - loss: 1.0387 - mean_squared_error: 1.0387\n",
        "      #25 в першому шарі і 24 в другому - loss: 0.9861 - mean_squared_error: 0.9861\n",
        "      #11 в першому шарі і 9 в другому - loss: 0.9711 - mean_squared_error: 0.9711\n",
        "\n",
        "hidden_dense_layer_1 = Dense(units=6, activation='relu', name='hidden_dense_layer_1')(preprocessing_layers)\n",
        "hidden_dense_layer_2 = Dense(units=4, activation='relu', name='hidden_dense_layer_2')(hidden_dense_layer_1)\n",
        "\n",
        "dense_output = tf.keras.layers.Dense(units=1, name='dense_output')(hidden_dense_layer_2)\n",
        "\n",
        "\n",
        "outputs = {\n",
        "    'dense_output': dense_output\n",
        "}\n",
        "\n",
        "\n",
        "multi_layer_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "multi_layer_model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mean_squared_error\", metrics=[MeanSquaredError()])\n",
        "\n",
        "multi_layer_history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                                epochs=epochs, shuffle=True, validation_split=validation_split)\n",
        "\n",
        "\n",
        "multi_layer_epochs = multi_layer_history.epoch\n",
        "multi_layer_hist = pd.DataFrame(multi_layer_history.history)\n",
        "multi_layer_mse = multi_layer_hist[\"mean_squared_error\"]\n",
        "\n",
        "plot_the_loss_curve(multi_layer_epochs, multi_layer_mse, multi_layer_hist[\"val_mean_squared_error\"])\n",
        "\n",
        "print(\"\\n Оцінінемо ОПТИМІЗОВАНУ багатошарову модель на тестовому наборі даних:\")\n",
        "multi_layer_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}